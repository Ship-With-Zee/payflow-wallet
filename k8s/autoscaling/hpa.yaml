# ============================================
# Horizontal Pod Autoscalers for PayFlow Services
# ============================================
# Purpose: Automatically scale pods based on CPU/Memory usage
# Ensures optimal resource utilization and handles traffic spikes
#
# How HPA Works:
# 1. HPA continuously monitors CPU and memory usage of pods
# 2. When metrics exceed target thresholds, HPA increases replicas
# 3. When metrics drop below thresholds, HPA decreases replicas
# 4. Scaling happens within min/max replica limits
#
# Prerequisites:
# - metrics-server must be enabled (microk8s enable metrics-server)
# - Deployments must have resource requests/limits defined
# - All PayFlow services already meet these requirements

# ============================================
# API Gateway HPA
# ============================================
# Why: API Gateway handles ALL incoming traffic, so it needs aggressive scaling
# Strategy: Fast scale-up, gradual scale-down to handle traffic spikes
apiVersion: autoscaling/v2  # Kubernetes API version for HPA v2 (supports multiple metrics)
kind: HorizontalPodAutoscaler  # Resource type - automatically scales pods horizontally
metadata:
  name: api-gateway-hpa  # Unique name for this HPA
  namespace: payflow  # Namespace where the deployment exists
spec:
  # ============================================
  # Scale Target Configuration
  # ============================================
  scaleTargetRef:
    apiVersion: apps/v1  # API version of the resource to scale
    kind: Deployment  # Type of resource (Deployment, StatefulSet, etc.)
    name: api-gateway  # Name of the deployment to scale
  
  # ============================================
  # Replica Limits
  # ============================================
  minReplicas: 2  # Minimum pods to maintain (ensures high availability)
  maxReplicas: 10  # Maximum pods allowed (prevents resource exhaustion)
  # Note: HPA will never scale below min or above max, regardless of metrics
  
  # ============================================
  # Scaling Metrics
  # ============================================
  # HPA scales when ANY metric exceeds its threshold
  # If CPU > 70% OR Memory > 80%, HPA will scale up
  metrics:
  - type: Resource  # Metric type: Resource (CPU/Memory) or Custom
    resource:
      name: cpu  # Resource to monitor: cpu or memory
      target:
        type: Utilization  # Target type: Utilization (percentage) or AverageValue
        averageUtilization: 70  # Target: Keep CPU usage at 70% or below
        # If current CPU > 70%, HPA calculates: desiredReplicas = ceil(currentReplicas * (currentCPU / 70))
  
  - type: Resource
    resource:
      name: memory  # Monitor memory usage
      target:
        type: Utilization
        averageUtilization: 80  # Target: Keep memory usage at 80% or below
        # Memory typically has less variance than CPU, so 80% is safe
  
  # ============================================
  # Scaling Behavior (Advanced)
  # ============================================
  # Controls how quickly and aggressively HPA scales
  behavior:
    # ============================================
    # Scale Down Behavior
    # ============================================
    scaleDown:
      stabilizationWindowSeconds: 300  # Wait 5 minutes before scaling down
      # Prevents rapid scale-down when metrics briefly drop
      # Example: If CPU drops to 50%, wait 5 min to confirm it stays low
      
      policies:
      - type: Percent  # Scale by percentage of current replicas
        value: 50  # Reduce by 50% at a time (e.g., 10 pods → 5 pods)
        periodSeconds: 60  # Apply this policy every 60 seconds
        # Gradual reduction prevents service disruption
    
    # ============================================
    # Scale Up Behavior
    # ============================================
    scaleUp:
      stabilizationWindowSeconds: 0  # No delay - scale up immediately
      # Fast scale-up is critical for handling traffic spikes
      # Don't wait when resources are exhausted
      
      policies:
      # Policy 1: Percentage-based scaling
      - type: Percent
        value: 100  # Double the pods (100% increase)
        periodSeconds: 60  # Can double every 60 seconds
        # Example: 2 pods → 4 pods → 8 pods (if needed)
      
      # Policy 2: Pod-based scaling
      - type: Pods
        value: 2  # Add 2 pods at a time
        periodSeconds: 60  # Can add 2 pods every 60 seconds
        # Example: 2 pods → 4 pods → 6 pods (if needed)
      
      selectPolicy: Max  # Use whichever policy scales MORE aggressively
      # If doubling (100%) adds more pods than adding 2, use doubling
      # This ensures rapid response to traffic spikes
---
# ============================================
# Auth Service HPA
# ============================================
# Why: Handles login/registration requests which can spike during peak times
# Strategy: Standard scaling (no custom behavior) - uses Kubernetes defaults
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: auth-service-hpa
  namespace: payflow
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: auth-service  # Targets the auth-service deployment
  
  minReplicas: 2  # Always maintain 2 pods for high availability
  maxReplicas: 8  # Can scale up to 8 pods during high authentication load
  
  # ============================================
  # Scaling Metrics
  # ============================================
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70  # Scale up if CPU > 70% across all pods
        # Auth operations (password hashing, JWT generation) are CPU-intensive
  
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80  # Scale up if Memory > 80% across all pods
        # Session storage and token caching use memory
---
# ============================================
# Wallet Service HPA
# ============================================
# Why: Handles balance checks, wallet updates, and balance queries
# Strategy: Standard scaling - wallet operations are relatively lightweight
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: wallet-service-hpa
  namespace: payflow
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: wallet-service  # Targets the wallet-service deployment
  
  minReplicas: 2  # Always maintain 2 pods for high availability
  maxReplicas: 8  # Can scale up to 8 pods during high wallet query load
  
  # ============================================
  # Scaling Metrics
  # ============================================
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70  # Scale up if CPU > 70%
        # Database queries and balance calculations use CPU
  
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80  # Scale up if Memory > 80%
        # Caching wallet balances in Redis uses memory
---
# ============================================
# Transaction Service HPA
# ============================================
# Note: Transaction Service HPA is already defined in deployments/transaction-service.yaml
# It's included in the same file as the deployment for convenience
# No need to duplicate it here to avoid conflicts
# Configuration: 2-10 replicas, CPU 70%, Memory 80%
---
# ============================================
# Notification Service HPA
# ============================================
# Why: Processes notifications from RabbitMQ queue
# Strategy: Moderate scaling - notifications are queued, so less urgent than API Gateway
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: notification-service-hpa
  namespace: payflow
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: notification-service  # Targets the notification-service deployment
  
  minReplicas: 2  # Always maintain 2 pods for high availability
  maxReplicas: 6  # Lower max than other services (notifications can queue)
  # Notifications are asynchronous - if queue builds up, we can process later
  
  # ============================================
  # Scaling Metrics
  # ============================================
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70  # Scale up if CPU > 70%
        # Processing RabbitMQ messages and sending emails/SMS uses CPU
  
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80  # Scale up if Memory > 80%
        # Message queue buffers and notification templates use memory
---
# ============================================
# Frontend HPA
# ============================================
# Why: Serves static React files and handles user interface requests
# Strategy: Moderate scaling - frontend is mostly static, but needs capacity for concurrent users
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: frontend-hpa
  namespace: payflow
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: frontend  # Targets the frontend deployment (Nginx serving React app)
  
  minReplicas: 2  # Always maintain 2 pods for high availability
  maxReplicas: 6  # Moderate max - frontend is lightweight (static files + Nginx)
  # Frontend pods are small and efficient, so 6 pods can handle significant traffic
  
  # ============================================
  # Scaling Metrics
  # ============================================
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70  # Scale up if CPU > 70%
        # Nginx serving static files is CPU-efficient, but concurrent connections add load
  
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80  # Scale up if Memory > 80%
        # Nginx buffers and connection handling use memory

